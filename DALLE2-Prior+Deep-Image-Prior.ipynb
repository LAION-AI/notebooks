{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: \n",
        "\n",
        "-- README/Citations goes here\n",
        "\n",
        "-- figure out 256px output (only does >= 512px right now)\n",
        "\n",
        "-- integrate settings capture for posting in dalle2-results\n",
        "\n",
        "-- probably fix the aesthetic loss function (aesthetic loss on cutouts)"
      ],
      "metadata": {
        "id": "fG4xyAeJ-nRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## obligatory nvidia-smi"
      ],
      "metadata": {
        "id": "b5Pk2lmC-r4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwrZUDbdoqpH",
        "outputId": "dc894f93-e513-4390-8cb6-829fd04bbc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 22 21:33:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## grab some stuff"
      ],
      "metadata": {
        "id": "9tuQw9mB-uIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oht-RgAIkykX",
        "outputId": "4fd0b662-cdf7-4d9f-ad3f-1224a37ef6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/nousr/DALLE2-pytorch.git\n",
            "  Cloning https://github.com/nousr/DALLE2-pytorch.git to /tmp/pip-req-build-atrqb82v\n",
            "  Running command git clone -q https://github.com/nousr/DALLE2-pytorch.git /tmp/pip-req-build-atrqb82v\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (7.1.2)\n",
            "Collecting clip-anytorch\n",
            "  Downloading clip_anytorch-2.4.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting coca-pytorch>=0.0.5\n",
            "  Downloading CoCa_pytorch-0.0.5-py3-none-any.whl (6.4 kB)\n",
            "Collecting einops>=0.4\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting einops-exts>=0.0.3\n",
            "  Downloading einops_exts-0.0.3-py3-none-any.whl (3.8 kB)\n",
            "Collecting embedding-reader\n",
            "  Downloading embedding_reader-1.4.2-py3-none-any.whl (17 kB)\n",
            "Collecting kornia>=0.5.4\n",
            "  Downloading kornia-0.6.5-py2.py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (7.1.2)\n",
            "Collecting resize-right>=0.0.2\n",
            "  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n",
            "Collecting rotary-embedding-torch\n",
            "  Downloading rotary_embedding_torch-0.1.5-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dalle2-pytorch==0.3.5) (4.64.0)\n",
            "Collecting vector-quantize-pytorch\n",
            "  Downloading vector_quantize_pytorch-0.6.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting x-clip>=0.4.4\n",
            "  Downloading x_clip-0.5.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting youtokentome\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting webdataset>=0.2.5\n",
            "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting fsspec>=2022.1.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia>=0.5.4->dalle2-pytorch==0.3.5) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10->dalle2-pytorch==0.3.5) (4.2.0)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from webdataset>=0.2.5->dalle2-pytorch==0.3.5) (3.13)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from x-clip>=0.4.4->dalle2-pytorch==0.3.5) (2019.12.20)\n",
            "Requirement already satisfied: pyarrow<8,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from embedding-reader->dalle2-pytorch==0.3.5) (6.0.1)\n",
            "Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from embedding-reader->dalle2-pytorch==0.3.5) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch==0.3.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch==0.3.5) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<2,>=1.1.5->embedding-reader->dalle2-pytorch==0.3.5) (1.15.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->x-clip>=0.4.4->dalle2-pytorch==0.3.5) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia>=0.5.4->dalle2-pytorch==0.3.5) (3.0.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->dalle2-pytorch==0.3.5) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->dalle2-pytorch==0.3.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->dalle2-pytorch==0.3.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->dalle2-pytorch==0.3.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->dalle2-pytorch==0.3.5) (2021.10.8)\n",
            "Building wheels for collected packages: dalle2-pytorch\n",
            "  Building wheel for dalle2-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dalle2-pytorch: filename=dalle2_pytorch-0.3.5-py3-none-any.whl size=1402867 sha256=aac118e409ccfe4e925d32b6ea3c1234f1ab79ebcac0c8086d6d562a044f0870\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ec9dwqv/wheels/9b/ec/9f/8b78188478a5a0ab8a24f23a5889a1dd754ccfb6b07cdb29b7\n",
            "Successfully built dalle2-pytorch\n",
            "Installing collected packages: ftfy, fsspec, einops, braceexpand, youtokentome, x-clip, webdataset, vector-quantize-pytorch, rotary-embedding-torch, resize-right, kornia, embedding-reader, einops-exts, coca-pytorch, clip-anytorch, dalle2-pytorch\n",
            "Successfully installed braceexpand-0.1.7 clip-anytorch-2.4.0 coca-pytorch-0.0.5 dalle2-pytorch-0.3.5 einops-0.4.1 einops-exts-0.0.3 embedding-reader-1.4.2 fsspec-2022.5.0 ftfy-6.1.1 kornia-0.6.5 resize-right-0.0.2 rotary-embedding-torch-0.1.5 vector-quantize-pytorch-0.6.0 webdataset-0.2.5 x-clip-0.5.1 youtokentome-1.0.6\n",
            "Collecting git+https://github.com/nousr/deep-image-prior.git\n",
            "  Cloning https://github.com/nousr/deep-image-prior.git to /tmp/pip-req-build-iumn4fuz\n",
            "  Running command git clone -q https://github.com/nousr/deep-image-prior.git /tmp/pip-req-build-iumn4fuz\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (3.13)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (2019.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (3.22.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (0.18.3)\n",
            "Requirement already satisfied: torch>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from deep-image-prior==0.0.1) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.2->deep-image-prior==0.0.1) (4.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->deep-image-prior==0.0.1) (2.21)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (7.7.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (5.3.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->deep-image-prior==0.0.1) (5.6.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep-image-prior==0.0.1) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep-image-prior==0.0.1) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep-image-prior==0.0.1) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->deep-image-prior==0.0.1) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->deep-image-prior==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep-image-prior==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep-image-prior==0.0.1) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep-image-prior==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->deep-image-prior==0.0.1) (5.4.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (4.10.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->deep-image-prior==0.0.1) (3.8.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep-image-prior==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep-image-prior==0.0.1) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->deep-image-prior==0.0.1) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->deep-image-prior==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->deep-image-prior==0.0.1) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->deep-image-prior==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->deep-image-prior==0.0.1) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep-image-prior==0.0.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep-image-prior==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deep-image-prior==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (from mkl->deep-image-prior==0.0.1) (2022.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (5.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->deep-image-prior==0.0.1) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->deep-image-prior==0.0.1) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->deep-image-prior==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->deep-image-prior==0.0.1) (21.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->deep-image-prior==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->deep-image-prior==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->deep-image-prior==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->deep-image-prior==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->deep-image-prior==0.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->deep-image-prior==0.0.1) (2021.10.8)\n",
            "Building wheels for collected packages: deep-image-prior\n",
            "  Building wheel for deep-image-prior (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deep-image-prior: filename=deep_image_prior-0.0.1-py3-none-any.whl size=24494 sha256=97b6768d1c6153cef983ad6e258f56d9dd9c4b6f3a6b0ac4c15ab6565bc2708d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ggt5kzgm/wheels/a3/58/2f/81409cc997124be990e0f71c55603287fa7fdfe42138c7f46a\n",
            "Successfully built deep-image-prior\n",
            "Installing collected packages: deep-image-prior\n",
            "Successfully installed deep-image-prior-0.0.1\n",
            "Collecting madgrad\n",
            "  Downloading madgrad-1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: madgrad\n",
            "Successfully installed madgrad-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/nousr/DALLE2-pytorch.git # can probaly switch to lucidrains, we can always revert to a hash if a breaking change is introduced\n",
        "!pip install git+https://github.com/nousr/deep-image-prior.git # pip-installable fork\n",
        "!pip install madgrad # for default deep image prior generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download models and style injection embedding"
      ],
      "metadata": {
        "id": "wgHnouztCbNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCbEjB8LlW73",
        "outputId": "3c4f803d-7102-4ea7-fcdd-020f82793f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-22 19:32:59--  https://huggingface.co/zenglishuci/conditioned-prior/resolve/main/vit-l-87k.pth\n",
            "Resolving huggingface.co (huggingface.co)... 34.197.58.156, 18.214.24.217, 34.232.89.2, ...\n",
            "Connecting to huggingface.co (huggingface.co)|34.197.58.156|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/82/5d/825dabd3e1765348e86eaa342efb982262672ddfc2273ec0d9ad7d589afd8587/da0e3ba64717705fc0439dfd5aa790a8c8da068a423fdfc75d9db487e27a2146?response-content-disposition=attachment%3B%20filename%3D%22vit-l-87k.pth%22 [following]\n",
            "--2022-05-22 19:32:59--  https://cdn-lfs.huggingface.co/repos/82/5d/825dabd3e1765348e86eaa342efb982262672ddfc2273ec0d9ad7d589afd8587/da0e3ba64717705fc0439dfd5aa790a8c8da068a423fdfc75d9db487e27a2146?response-content-disposition=attachment%3B%20filename%3D%22vit-l-87k.pth%22\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.18.102, 52.84.18.121, 52.84.18.77, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.18.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2200498685 (2.0G) [application/octet-stream]\n",
            "Saving to: ‘vit-l-87k.pth’\n",
            "\n",
            "vit-l-87k.pth       100%[===================>]   2.05G  48.4MB/s    in 57s     \n",
            "\n",
            "2022-05-22 19:33:56 (37.1 MB/s) - ‘vit-l-87k.pth’ saved [2200498685/2200498685]\n",
            "\n",
            "--2022-05-22 19:33:56--  https://github.com/LAION-AI/aesthetic-predictor/raw/main/vit_l_14_embeddings/rating9.npy\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LAION-AI/aesthetic-predictor/main/vit_l_14_embeddings/rating9.npy [following]\n",
            "--2022-05-22 19:33:56--  https://raw.githubusercontent.com/LAION-AI/aesthetic-predictor/main/vit_l_14_embeddings/rating9.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [application/octet-stream]\n",
            "Saving to: ‘rating9.npy’\n",
            "\n",
            "rating9.npy         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-22 19:33:56 (59.7 MB/s) - ‘rating9.npy’ saved [3200/3200]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/zenglishuci/conditioned-prior/resolve/main/vit-l-87k.pth # 100k checkpoint may have overfit a bit, use 87k for now\n",
        "!wget https://github.com/LAION-AI/aesthetic-predictor/raw/main/vit_l_14_embeddings/rating9.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "tqje5vW6-w1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "import gc\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from einops import rearrange\n",
        "import kornia.augmentation as K\n",
        "from madgrad import MADGRAD\n",
        "from resize_right import resize\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork,OpenAIClipAdapter, DiffusionPriorTrainer\n",
        "\n",
        "from deep_image_prior.models import get_hq_skip_net, get_non_offset_params, get_offset_params, skip\n",
        "from deep_image_prior.utils import sr_utils\n",
        "\n",
        "from clip import tokenize\n",
        "\n",
        "# for resize right floor division\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from os.path import expanduser  # pylint: disable=import-outside-toplevel\n",
        "from urllib.request import urlretrieve  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "zP2oUCJMkOhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe4502b-6207-4aa5-c6e1-71c6e852aa51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "l85IcVA5-yoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Experimental Stuff\n",
        "# ------------------------------------------------------------------------------\n",
        "def inject_style(emb, style, weight=0.5):\n",
        "  summation = (emb + weight*style)\n",
        "  summation_norm = summation.norm()\n",
        "  return summation/summation_norm\n",
        "\n",
        "def get_aesthetic_model(clip_model=\"vit_l_14\"):\n",
        "    \"\"\"load the aethetic model\"\"\"\n",
        "    home = expanduser(\"~\")\n",
        "    cache_folder = home + \"/.cache/emb_reader\"\n",
        "    path_to_model = cache_folder + \"/sa_0_4_\"+clip_model+\"_linear.pth\"\n",
        "    if not os.path.exists(path_to_model):\n",
        "        os.makedirs(cache_folder, exist_ok=True)\n",
        "        url_model = (\n",
        "            \"https://github.com/LAION-AI/aesthetic-predictor/blob/main/sa_0_4_\"+clip_model+\"_linear.pth?raw=true\"\n",
        "        )\n",
        "        urlretrieve(url_model, path_to_model)\n",
        "    if clip_model == \"vit_l_14\":\n",
        "        m = nn.Linear(768, 1)\n",
        "    elif clip_model == \"vit_b_32\":\n",
        "        m = nn.Linear(512, 1)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "    s = torch.load(path_to_model)\n",
        "    m.load_state_dict(s)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "def aesthetic_loss_cutouts(cutouts, aesthetic_model, weight=0.05):\n",
        "  \"\"\"\n",
        "  Perform aesthetic loss on the cutouts\n",
        "\n",
        "  TODO: can probably done way better, just wanted to block this functionality in\n",
        "  \"\"\"\n",
        "  aesthetic_losses = 0\n",
        "  for cutout in cutouts:\n",
        "    aesthetic_losses += aesthetic_model(cutout).item()\n",
        "  \n",
        "  # average out over all cutouts\n",
        "  aesthetic_losses /= cutouts.shape[0]\n",
        "\n",
        "  # clip if below zero\n",
        "  if aesthetic_losses <= 0:\n",
        "    aesthetic_losses = 1e-16\n",
        "\n",
        "  return weight/aesthetic_losses\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Usual clip-guidance functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    \"\"\"\n",
        "    from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
        "    \"\"\"\n",
        "    import random, os\n",
        "    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = True\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "\n",
        "def clamp(x, min_value, max_value):\n",
        "    return max(min(x, max_value), min_value)\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        # self.cut_pow = cut_pow\n",
        "        self.augs = T.Compose([\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.8, padding_mode='border', resample='bilinear'),\n",
        "            K.RandomPerspective(0.4, p=0.7, resample='bilinear'),\n",
        "            K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7),\n",
        "            K.RandomGrayscale(p=0.15),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        long_size, short_size = max(sideX, sideY), min(sideX, sideY)\n",
        "        min_size = min(short_size, self.cut_size)\n",
        "        pad_x, pad_y = long_size - sideX, long_size - sideY\n",
        "        input_zero_padded = F.pad(input, (pad_x, pad_x, pad_y, pad_y), 'constant')\n",
        "        input_mask = F.pad(torch.zeros_like(input), (pad_x, pad_x, pad_y, pad_y), 'constant', 1.)\n",
        "        input_padded = input_zero_padded + input_mask * input.mean(dim=[2, 3], keepdim=True)\n",
        "        cutouts = []\n",
        "        for cn in range(self.cutn):\n",
        "            if cn >= self.cutn - self.cutn // 4:\n",
        "                size = long_size\n",
        "            else:\n",
        "                size = clamp(int(short_size * torch.zeros([]).normal_(mean=.8, std=.3)), min_size, long_size)\n",
        "            # size = int(torch.rand([])**self.cut_pow * (short_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(min(0, sideX - size), abs(sideX - size) + 1, ()) + pad_x\n",
        "            offsety = torch.randint(min(0, sideY - size), abs(sideY - size) + 1, ()) + pad_y\n",
        "            cutout = input_padded[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            # cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "            cutouts.append(resize(cutout, out_shape=(self.cut_size, self.cut_size),by_convs=True, pad_mode='reflect'))\n",
        "        return self.augs(torch.cat(cutouts))\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "class CaptureOutput:\n",
        "    \"\"\"Captures a layer's output activations using a forward hook.\"\"\"\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.output = None\n",
        "        self.handle = module.register_forward_hook(self)\n",
        "\n",
        "    def __call__(self, module, input, output):\n",
        "        self.output = output\n",
        "\n",
        "    def __del__(self):\n",
        "        self.handle.remove()\n",
        "\n",
        "    def get_output(self):\n",
        "        return self.output\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Model loading \n",
        "# ------------------------------------------------------------------------------\n",
        "def load_dip(input_depth, num_scales, offset_type, offset_groups, device):\n",
        "    dip_net = get_hq_skip_net(\n",
        "        input_depth,\n",
        "        skip_n33d=192,\n",
        "        skip_n33u=192,\n",
        "        skip_n11=4,\n",
        "        num_scales=num_scales,\n",
        "        offset_type=offset_type,\n",
        "        offset_groups=offset_groups\n",
        "    ).to(device)\n",
        "\n",
        "    return dip_net\n",
        "\n",
        "def load_diffusion_model(dprior_path, device, clip_choice):\n",
        "\n",
        "    loaded_obj = torch.load(str(dprior_path), map_location='cpu')\n",
        "    \n",
        "    if clip_choice == \"vit_b_32\":\n",
        "      dim = 512\n",
        "      clip_choice = \"ViT-B/32\"\n",
        "    else:\n",
        "      dim = 768\n",
        "      clip_choice = \"ViT-L/14\"\n",
        "\n",
        "    prior_network = DiffusionPriorNetwork(\n",
        "        dim=dim,\n",
        "        depth=12,\n",
        "        dim_head=64,\n",
        "        heads=12,\n",
        "        normformer=True\n",
        "    ).to(device)\n",
        "\n",
        "    diffusion_prior = DiffusionPrior(\n",
        "        net=prior_network,\n",
        "        clip=OpenAIClipAdapter(clip_choice),\n",
        "        image_embed_dim=dim,\n",
        "        timesteps=1000,\n",
        "        cond_drop_prob=0.1,\n",
        "        loss_type=\"l2\",\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    diffusion_prior.load_state_dict(loaded_obj[\"model\"], strict=True)\n",
        "\n",
        "    diffusion_prior = DiffusionPriorTrainer(\n",
        "                      diffusion_prior = diffusion_prior,\n",
        "                      lr = 1.1e-4,\n",
        "                      wd = 6.02e-2,\n",
        "                      max_grad_norm = 0.5,\n",
        "                      amp = False,\n",
        "                  ).to(device)\n",
        "\n",
        "    diffusion_prior.optimizer.load_state_dict(loaded_obj['optimizer'])\n",
        "    diffusion_prior.scaler.load_state_dict(loaded_obj['scaler'])\n",
        "\n",
        "    return diffusion_prior\n",
        "# ------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "_dhXZTYxkSGW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load static models"
      ],
      "metadata": {
        "id": "ozCFxV3I-5wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_choice = \"vit_l_14\"\n",
        "aesthetic_model = get_aesthetic_model(clip_model=clip_choice).to(device)\n",
        "print(\"loaded aesthetic model\")\n",
        "diffusion_prior = load_diffusion_model(\"vit-l-87k.pth\", 'cuda', clip_choice).eval().requires_grad_(False)\n",
        "print(\"loaded model\")\n",
        "\n",
        "# load the style embedding\n",
        "aesthetic_embedding = torch.from_numpy(np.load(\"rating9.npy\")).to(device)"
      ],
      "metadata": {
        "id": "XFC9m1oCT6-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382e97b4-0529-4bce-8fba-ed4e847748f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded aesthetic model\n",
            "loaded model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙ Settings "
      ],
      "metadata": {
        "id": "5CN1D4E5_KoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden settings for nerds\n",
        "offset_type = 'none'\n",
        "num_scales = 7\n",
        "lr = 1e-3\n",
        "offset_lr_fac = 1.0\n",
        "lr_decay= 0.995\n",
        "input_depth = 32\n",
        "optimizer_type = 'MADGRAD'  # Adam, MADGRAD\n",
        "clip_size = 224 # can probably stay like this unless we train a ViT-L/14@336\n",
        "\n",
        "# @markdown ### Generation Settings\n",
        "# @markdown\n",
        "cutn = 20 #@param {type:\"integer\"}\n",
        "size = [512, 512] #@param [\"[512, 512]\", \"[768, 512]\", \"[512, 768]\"] {type:\"raw\", allow-input: true}\n",
        "iterations = 500 #@param {type:\"integer\"}\n",
        "display_freq = 25 #@param {type:\"integer\"}\n",
        "disable_deformable_convolutions = False #@param {type:\"boolean\"}\n",
        "seed = 0 #@param {type: \"integer\"}\n",
        "\n",
        "# TODO: add seeding\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Experimental Settings\n",
        "# @markdown\n",
        "use_aesthetic_loss = True #@param {type:\"boolean\"}\n",
        "aesthetic_loss_weight = 0.05 #@param {type:\"slider\", min:0, max:0.1, step:0.01}\n",
        "inject_style_embedding = True #@param {type:\"boolean\"}\n",
        "style_injection_weight = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Additonal Settings\n",
        "# @markdown\n",
        "input_scale = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "input_noise_strength = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "param_noise_strength = 0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# instantiate variables from specified settings\n",
        "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                        std=[0.26862954, 0.26130258, 0.27577711])\n",
        "sideX, sideY = size\n",
        "\n",
        "# set seeds\n",
        "seed_everything(seed)\n",
        "\n",
        "# disable deform convs\n",
        "offset_groups = 0 if disable_deformable_convolutions else 4"
      ],
      "metadata": {
        "id": "jGyJUHzyH4F5",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎨 Actually do the run!"
      ],
      "metadata": {
        "id": "O2gYS1p-_Mp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nu5f5otjkdc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate\n",
        "prompt = \"A beautiful painting of sailboats, in the style of greg rutkowski\" #@param {type:\"string\"}\n",
        "tokenized_text = tokenize(prompt).to(device)\n",
        "\n",
        "# Initialize DIP skip network \n",
        "net = load_dip(input_depth=input_depth,\n",
        "               num_scales=num_scales, \n",
        "               offset_type=offset_type, \n",
        "               offset_groups=offset_groups, \n",
        "               device=device)\n",
        "# init the cutouts\n",
        "make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "\n",
        "# Initialize input noise\n",
        "net_input = torch.randn([1, input_depth, sideY, sideX], device=device)\n",
        "\n",
        "\n",
        "noise = torch.randn((1,768))\n",
        "t = torch.linspace(1, 0, 1000 + 1)[:-1]\n",
        "\n",
        "score = 0\n",
        "target_embed = diffusion_prior.sample(tokenized_text, num_samples_per_batch=2)\n",
        "score = aesthetic_model(target_embed).item()\n",
        "print(f\"\\n>> Aesthetic without injecting style: {score}\")\n",
        "\n",
        "if inject_style_embedding == True:\n",
        "  target_embed = inject_style(target_embed, aesthetic_embedding, weight=style_injection_weight)\n",
        "  score = aesthetic_model(target_embed).item()\n",
        "  print(f\">> Aesthetic score after injecting style: {score}\\n\")\n",
        "\n",
        "prompts = [Prompt(target_embed)]\n",
        "\n",
        "params = [{'params': get_non_offset_params(net), 'lr': lr},\n",
        "          {'params': get_offset_params(net), 'lr': lr * offset_lr_fac}]\n",
        "\n",
        "if optimizer_type == 'Adam':\n",
        "    opt = optim.Adam(params, lr)\n",
        "elif optimizer_type == 'MADGRAD':\n",
        "    opt = MADGRAD(params, lr, momentum=0.9)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "image = None\n",
        "\n",
        "itt = 0\n",
        "try:\n",
        "    for _ in trange(iterations, leave=True, position=0, colour='#DD6E42'):\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        noise_ramp = 1 - min(1, itt / iterations)\n",
        "        net_input_noised = net_input\n",
        "\n",
        "        if input_noise_strength:\n",
        "            phi = min(1, noise_ramp * input_noise_strength) * math.pi / 2\n",
        "            noise = torch.randn_like(net_input)\n",
        "            net_input_noised = net_input * math.cos(phi) + noise * math.sin(phi)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = net(net_input_noised * input_scale).float()\n",
        "\n",
        "        losses = []\n",
        "        cutouts = normalize(make_cutouts(out))\n",
        "        \n",
        "        with torch.cuda.amp.autocast(False):\n",
        "            image_embeds = diffusion_prior.diffusion_prior.clip.clip.encode_image(cutouts).float()\n",
        "        \n",
        "        for prompt in prompts:\n",
        "\n",
        "            if use_aesthetic_loss:\n",
        "              # print(f\"Aesthetic loss of cutouts: {aesthetics}\")\n",
        "              aesthetics = aesthetic_loss_cutouts(image_embeds, aesthetic_model=aesthetic_model, weight=aesthetic_loss_weight)\n",
        "              losses.append(prompt(image_embeds) + aesthetics)\n",
        "            else:\n",
        "              losses.append(prompt(image_embeds))\n",
        "\n",
        "        loss = sum(losses, out.new_zeros([]))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        if param_noise_strength:\n",
        "            with torch.no_grad():\n",
        "                noise_ramp = 1 - min(1, itt / iterations)\n",
        "                for group in opt.param_groups:\n",
        "                    for param in group['params']:\n",
        "                        param += torch.randn_like(param) * group['lr'] * param_noise_strength * noise_ramp\n",
        "\n",
        "        itt += 1\n",
        "\n",
        "        if itt % display_freq == 0 :\n",
        "            with torch.inference_mode():\n",
        "                image = TF.to_pil_image(out[0].clamp(0, 1))\n",
        "                if itt % display_freq == 0:\n",
        "                    losses_str = ', '.join([f'{loss.item():g}' for loss in losses])\n",
        "                    tqdm.write(f'i: {itt}, loss: {loss.item():g}, losses: {losses_str}')\n",
        "                    image.save(f'out_{itt:05}.png')\n",
        "                    display(image, display_id=1)\n",
        "        for group in opt.param_groups:\n",
        "            group['lr'] = lr_decay * group['lr']\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if net is not None:\n",
        "      del net\n",
        "    pass\n",
        "\n",
        "del net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cTzejxC7AQTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b5Pk2lmC-r4T",
        "9tuQw9mB-uIL",
        "wgHnouztCbNo",
        "tqje5vW6-w1e",
        "ozCFxV3I-5wp"
      ],
      "machine_shape": "hm",
      "name": "DALLE2 Prior + Deep Image Prior.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}